@inproceedings{Shivashankar2012,
  title={A hierarchical goal-based formalism and algorithm for single-agent planning},
  author={Shivashankar, Vikas and Kuter, Ugur and Nau, Dana and Alford, Ron},
  booktitle={Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume 2},
  pages={981--988},
  year={2012},
  organization={International Foundation for Autonomous Agents and Multiagent Systems}
}


@article{Cushman2015,
author = {Cushman, Fiery and Morris, Adam}, 
title = {Habitual control of goal selection in humans},
volume = {112}, 
number = {45}, 
pages = {13817-13822}, 
year = {2015}, 
doi = {10.1073/pnas.1506367112}, 
abstract ={Humans choose actions based on both habit and planning. Habitual control is computationally frugal but adapts slowly to novel circumstances, whereas planning is computationally expensive but can adapt swiftly. Current research emphasizes the competition between habits and plans for behavioral control, yet many complex tasks instead favor their integration. We consider a hierarchical architecture that exploits the computational efficiency of habitual control to select goals while preserving the flexibility of planning to achieve those goals. We formalize this mechanism in a reinforcement learning setting, illustrate its costs and benefits, and experimentally demonstrate its spontaneous application in a sequential decision-making task.}, 
journal = {Proceedings of the National Academy of Sciences} 
}


@inproceedings{ng99,
author = {Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
booktitle = {ICML},
pages = {278--287},
title = {{Policy invariance under reward transformations: Theory and application to reward shaping}},
volume = {99},
year = {1999}
}



@article{Myerson1995,
    abstract = {The present paper addresses the question of the form of the mathematical relation between the time until a delayed reward and its present value. Data are presented from an experiment in which subjects chose between hypothetical amounts of money available either immediately or after a delay (Green, Fry, \& Myerson, 1994). Analyses of the behavior of individual young adults demonstrated that temporal discounting is better described by hyperbola-like functions than by exponential decay functions. For most individuals, the parameter that determines the rate of discounting varied inversely with amount. Raising the denominator of the discounting function to a power resulted in better descriptions of the data from most subjects. Two possible derivations of the temporal discounting function are proposed, a repeated choice model and an expected value model. These provide theoretical interpretations for amount-dependent discounting but amount-independent exponent parameters.},
    author = {Myerson, J. and Green, L.},
    issn = {0022-5002},
    journal = {Journal of the experimental analysis of behavior},
    keywords = {decision\_making, intertemporal\_choice},
    number = {3},
    pages = {263--276},
    pmcid = {PMC1350137},
    pmid = {16812772},
    posted-at = {2015-10-28 23:16:37},
    priority = {2},
    title = {Discounting of delayed rewards: Models of individual choice.},
    volume = {64},
    year = {1995}
}

@article{bargh01a,
  title={The automated will: nonconscious activation and pursuit of behavioral goals.},
  author={Bargh, John A and Gollwitzer, Peter M and Lee-Chai, Annette and Barndollar, Kimberly and Tr{\"o}tschel, Roman},
  journal={Journal of personality and social psychology},
  volume={81},
  number={6},
  pages={1014},
  year={2001},
  publisher={American Psychological Association}
}

@article{Gureckis2016,
  title={psi{T}urk: An open-source framework for conducting replicable behavioral experiments online},
  author={Gureckis, Todd M and Martin, Jay and McDonnell, John and Rich, Alexander S and Markant, Doug and Coenen, Anna and Halpern, David and Hamrick, Jessica B and Chan, Patricia},
  journal={Behavior research methods},
  volume={48},
  number={3},
  pages={829--842},
  year={2016},
  publisher={Springer}
}

@article{de-leeuw15,
  title={js{P}sych: A JavaScript library for creating behavioral experiments in a Web browser},
  author={De Leeuw, Joshua R},
  journal={Behavior Research Methods},
  volume={47},
  number={1},
  pages={1--12},
  year={2015},
  publisher={Springer}
}

@inproceedings{LiederGriffiths2016,
  title={Helping people make better decisions using optimal gamification},
  author={Lieder, Falk and Griffiths, Thomas L},
  booktitle={Proceedings of the 38th Annual Meeting of the Cognitive Science Society},
  publisher={Cognitive Science Society},
  address={Austin, TX},
  pages={2075--80},
  year={2016}
}

@inproceedings{lieder2012,
  title={Burn-in, bias, and the rationality of anchoring},
  author={Lieder, Falk and Griffiths, Thomas and Goodman, Noah},
  booktitle={Advances in neural information processing systems 25},
  editor={P. Bartlett, F.C.N. Pereira, Leon Bottou, Chris J.C. Burges, & K.Q. Weinberger},
  pages={2690--2798},
  year={2012}
}

@article{ferguson04,
  title={Liking is for doing: the effects of goal pursuit on automatic evaluation.},
  author={Ferguson, Melissa J and Bargh, John A},
  journal={Journal of personality and social psychology},
  volume={87},
  number={5},
  pages={557},
  year={2004},
  publisher={American Psychological Association}
}

@inproceedings{Marthi2008,
  title={Angelic Hierarchical Planning: Optimal and Online Algorithms.},
  author={Marthi, Bhaskara and Russell, Stuart J and Wolfe, Jason},
  booktitle={ICAPS},
  pages={222--231},
  year={2008}
}

@article{Simon1972,
  title={Theories of bounded rationality},
  author={Simon, Herbert A},
  journal={Decision and organization},
  volume={1},
  number={1},
  pages={161--176},
  year={1972},
  publisher={Amsterdam: North Holland}
}

@inproceedings{CallawayLiederKrueger2017,
title={Mouselab-{MDP}: A new paradigm for tracing how people plan},
author={Callaway, F. and Lieder, F. and Krueger, P. M. and Griffiths, T. L.},
booktitle={The 3rd Multidisciplinary Conference on
Reinforcement Learning and Decision Making},
year={in press}
}

@article{Russell1995,
  title={Provably bounded-optimal agents},
  author={Russell, Stuart J and Subramanian, Devika},
  journal={Journal of Artificial Intelligence Research},
  volume={2},
  pages={575--609},
  year={1995}
}

@article {SimonDaw2011,
	author = {Simon, Dylan Alexander and Daw, Nathaniel D.},
	title = {Neural Correlates of Forward Planning in a Spatial Decision Task in Humans},
	volume = {31},
	number = {14},
	pages = {5526--5539},
	year = {2011},
	publisher = {Society for Neuroscience},
	abstract = {Although reinforcement learning (RL) theories have been influential in characterizing the mechanisms for reward-guided choice in the brain, the predominant temporal difference (TD) algorithm cannot explain many flexible or goal-directed actions that have been demonstrated behaviorally. We investigate such actions by contrasting an RL algorithm that is model based, in that it relies on learning a map or model of the task and planning within it, to traditional model-free TD learning. To distinguish these approaches in humans, we used functional magnetic resonance imaging in a continuous spatial navigation task, in which frequent changes to the layout of the maze forced subjects continually to relearn their favored routes, thereby exposing the RL mechanisms used. We sought evidence for the neural substrates of such mechanisms by comparing choice behavior and blood oxygen level-dependent (BOLD) signals to decision variables extracted from simulations of either algorithm. Both choices and value-related BOLD signals in striatum, although most often associated with TD learning, were better explained by the model-based theory. Furthermore, predecessor quantities for the model-based value computation were correlated with BOLD signals in the medial temporal lobe and frontal cortex. These results point to a significant extension of both the computational and anatomical substrates for RL in the brain.},
	issn = {0270-6474},
	journal = {Journal of Neuroscience}
}

@article{De2015jspsych,
  title={jsPsych: A JavaScript library for creating behavioral experiments in a Web browser},
  author={De Leeuw, Joshua R},
  journal={Behavior Research Methods},
  volume={47},
  number={1},
  pages={1--12},
  year={2015},
  publisher={Springer}
}

@book{Miller1986,
  title={Plans and the structure of behavior.},
  author={Miller, George A and Galanter, Eugene and Pribram, Karl H},
  year={1986},
  publisher={Adams Bannister Cox}
}

@article{Huys2012,
  title={Bonsai trees in your head: how the Pavlovian system sculpts goal-directed choices by pruning decision trees},
  author={Huys, Quentin JM and Eshel, Neir and O'Nions, Elizabeth and Sheridan, Luke and Dayan, Peter and Roiser, Jonathan P},
  journal={PLoS Comput Biol},
  volume={8},
  number={3},
  pages={e1002410},
  year={2012},
  publisher={Public Library of Science}
}

@article{Huys2015,
  title={Interplay of approximate planning strategies},
  author={Huys, Quentin JM and Lally, N{\'\i}all and Faulkner, Paul and Eshel, Neir and Seifritz, Erich and Gershman, Samuel J and Dayan, Peter and Roiser, Jonathan P},
  journal={Proceedings of the National Academy of Sciences},
  volume={112},
  number={10},
  pages={3098--3103},
  year={2015},
  publisher={National Acad Sciences}
}

@article{Botvinick2009,
  title={Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective},
  author={Botvinick, Matthew M and Niv, Yael and Barto, Andrew C},
  journal={Cognition},
  volume={113},
  number={3},
  pages={262--280},
  year={2009},
  publisher={Elsevier}
}

@book{NewellSimon1972,
  title={Human problem solving},
  author={Newell, Allen and Simon, Herbert Alexander and others},
  volume={104},
  number={9},
  year={1972},
  publisher={Prentice-Hall Englewood Cliffs, NJ}
}

@article{Balaguer2016,
  title={Neural mechanisms of hierarchical planning in a virtual subway network},
  author={Balaguer, Jan and Spiers, Hugo and Hassabis, Demis and Summerfield, Christopher},
  journal={Neuron},
  volume={90},
  number={4},
  pages={893--903},
  year={2016},
  publisher={Elsevier}
}

@book{Morris2004,
  title={The cognitive psychology of planning},
  author={Morris, Robin and Ward, Geoff},
  year={2004},
  publisher={Psychology Press}
}

@article{Olafsdottir2015,
  title={Hippocampal place cells construct reward related sequences through unexplored space},
  author={{\'O}lafsd{\'o}ttir, H Freyja and Barry, Caswell and Saleem, Aman B and Hassabis, Demis and Spiers, Hugo J},
  journal={Elife},
  volume={4},
  pages={e06063},
  year={2015},
  publisher={eLife Sciences Publications Limited}
}


@article{Horvitz2001Computational,
    author = {Horvitz, Eric and Zilberstein, Shlomo},
    citeulike-article-id = {13159883},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/s0004-3702(01)00051-0},
    doi = {10.1016/s0004-3702(01)00051-0},
    issn = {00043702},
    journal = {Artificial Intelligence},
    keywords = {ai, algorithm\_selection, bounded-rationality, bounded\_optimality, rationality, speed-accuracy\_tradeoff},
    month = feb,
    number = {1-2},
    pages = {1--4},
    posted-at = {2014-05-06 00:56:16},
    priority = {2},
    title = {Computational tradeoffs under bounded resources},
    url = {http://dx.doi.org/10.1016/s0004-3702(01)00051-0},
    volume = {126},
    year = {2001}
}

@techreport{Kunz2009,
    author = {Kunz, Simon},
    citeulike-article-id = {13477366},
    institution = {Seminar for Statistics, ETH Zurich, Switzerland},
    keywords = {bayesian, bayesian\_inference, bayesian\_statistics, machine\_learning, regression},
    posted-at = {2015-01-08 01:20:32},
    priority = {2},
    title = {The {B}ayesian Linear model with Unknown Variance},
    year = {2009}
}

@article{LindleySmith1972,
    abstract = {The usual linear statistical model is reanalyzed using Bayesian methods and the concept of exchangeability. The general method is illustrated by applications to two-factor experimental designs and multiple regression.},
    author = {Lindley, D. V. and Smith, A. F. M.},
    citeulike-article-id = {13477369},
    citeulike-linkout-0 = {http://www.jstor.org/stable/2985048},
    journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
    keywords = {bayesian, bayesian\_inference, bayesian\_statistics, machine\_learning, regression, statistics},
    number = {1},
    posted-at = {2015-01-08 01:23:58},
    priority = {3},
    publisher = {Wiley for the Royal Statistical Society},
    title = {Bayes Estimates for the Linear Model},
    volume = {34},
    year = {1972}
}

@article{Gonzalez2011,
author={Gonzalez, C. and Dutt, V.},
year = {2011},
title= {Instance-based learning: Integrating sampling and repeated decisions from experience},
journal= {Psychological Review},
volume={118},
number={4},
pages={523-551}
}

@article{Gomes2001Algorithm,
    abstract = {Stochastic algorithms are among the best methods for solving computationally hard search and reasoning problems. The run time of such procedures can vary significantly from instance to instance and, when using different random seeds, on the same instance. One can take advantage of such differences by combining several algorithms into a portfolio, and running them in parallel or interleaving them on a single processor. We provide an evaluation of the portfolio approach on distributions of hard combinatorial search problems. We show under what conditions the portfolio approach can have a dramatic computational advantage over the best traditional methods. In particular, we will see how, in a portfolio setting, it can be advantageous to use a more  ” risk-seeking” strategy with a high variance in run time, such as a randomized depth-first search approach in mixed integer programming versus the more traditional best-bound approach. We hope these insights will stimulate the development of novel randomized combinatorial search methods.},
    address = {Essex, UK},
    author = {Gomes, Carla P. and Selman, Bart},
    citeulike-article-id = {797118},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=370612},
    citeulike-linkout-1 = {http://dx.doi.org/10.1016/s0004-3702(00)00081-3},
    citeulike-linkout-2 = {http://www.sciencedirect.com/science/article/B6TYF-42KDH2N-3/2/701edb8faddbd2b07a88525787e5bef3},
    doi = {10.1016/s0004-3702(00)00081-3},
    issn = {00043702},
    journal = {Artificial Intelligence},
    keywords = {ai, algorithm\_portfolios, algorithm\_selection, reasoning, search},
    number = {1-2},
    pages = {43--62},
    posted-at = {2014-05-06 00:54:58},
    priority = {2},
    publisher = {Elsevier Science Publishers Ltd.},
    title = {Algorithm portfolios},
    url = {http://dx.doi.org/10.1016/s0004-3702(00)00081-3},
    volume = {126},
    year = {2001}
}

@article{Petrik2006Learning,
    abstract = {A wide range of combinatorial optimization algorithms have been developed for complex reasoning tasks. Frequently, no single algorithm outperforms all the others. This has raised interest in leveraging the performance of a collection of algorithms to improve performance. We show how to accomplish this using a Parallel Portfolio of Algorithms (PPA). A PPA is a collection of diverse algorithms for solving a single problem, all running concurrently on a single processor until a solution is produced. The performance of the portfolio may be controlled by assigning different shares of processor time to each algorithm. We present an effective method for finding a PPA in which the share of processor time allocated to each algorithm is fixed. Finding the optimal static schedule is shown to be an NP-complete problem for a general class of utility functions. We present bounds on the performance of the PPA over random instances and evaluate the performance empirically on a collection of 23 state-of-the-art SAT algorithms. The results show significant performance gains over the fastest individual algorithm in the collection.},
    author = {Petrik, Marek and Zilberstein, Shlomo},
    booktitle = {Annals of Mathematics and Artificial Intelligence},
    citeulike-article-id = {1507696},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1265464},
    citeulike-linkout-1 = {http://dx.doi.org/10.1007/s10472-007-9050-9},
    citeulike-linkout-2 = {http://www.ingentaconnect.com/content/klu/amai/2006/00000048/F0020001/00009050},
    citeulike-linkout-3 = {http://www.springerlink.com/content/n2v48154761661kk},
    citeulike-linkout-4 = {http://link.springer.com/article/10.1007/s10472-007-9050-9},
    day = {1},
    doi = {10.1007/s10472-007-9050-9},
    issn = {1012-2443},
    journal = {Annals of Mathematics and Artificial Intelligence},
    keywords = {ai, algorithm\_selection, combinatorial\_optimization, reasoning},
    number = {1-2},
    pages = {85--106},
    posted-at = {2014-05-06 00:49:48},
    priority = {2},
    publisher = {Kluwer Academic Publishers},
    title = {Learning parallel portfolios of algorithms},
    url = {http://dx.doi.org/10.1007/s10472-007-9050-9},
    volume = {48},
    year = {2006}
}

@article{Luby1993Optimal,
    author = {Luby, Michael and Sinclair, Alistair and Zuckerman, David},
    citeulike-article-id = {13159882},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/0020-0190(93)90029-9},
    doi = {10.1016/0020-0190(93)90029-9},
    issn = {00200190},
    journal = {Information Processing Letters},
    keywords = {ai, algorithm\_selection, machine\_learning, metacontrol},
    number = {4},
    pages = {173--180},
    posted-at = {2014-05-06 00:45:37},
    priority = {2},
    title = {Optimal speedup of Las Vegas algorithms},
    url = {http://dx.doi.org/10.1016/0020-0190(93)90029-9},
    volume = {47},
    year = {1993}
}

@inproceedings{Lagoudakis2001,
    author = {Lagoudakis, Michail G. and Littman, Michael L. and Parr, Ronald},
    booktitle = {Proceedings of the 2001 AAAI Fall Symposium Series: Using Uncertainty within Computation, Cape Cod, MA},
    citeulike-article-id = {13137002},
    keywords = {ai, algorithm\_selection, machine\_learning, mdp, metalearning, metareasoning, sorting},
    posted-at = {2014-04-13 05:45:42},
    priority = {2},
    title = {Selecting the right algorithm},
    year = {2001}
}

@inproceedings{Lagoudakis2000Reinforcement,
    author = {Lagoudakis, Michail G. and Littman, Michael L.},
    booktitle = {AAAI/IAAI},
    citeulike-article-id = {2137174},
    citeulike-linkout-0 = {http://dblp.uni-trier.de/rec/bibtex/conf/aaai/LagoudakisL00},
    keywords = {ai, algorithm\_selection, machine\_learning, reinforcement\_learning},
    posted-at = {2014-03-28 17:38:10},
    priority = {2},
    publisher = {AAAI Press / The MIT Press},
    title = {Reinforcement Learning for Algorithm Selection},
    url = {http://dblp.uni-trier.de/rec/bibtex/conf/aaai/LagoudakisL00},
    year = {2000}
}

@incollection{Gagliolo2004Adaptive,
    abstract = {Given is a search problem or a sequence of search problems, as well as a set of potentially useful search algorithms. We propose a general framework for online allocation of computation time to search algorithms based on experience with their performance so far. In an example instantiation, we use simple linear extrapolation of performance for allocating time to various simultaneously running genetic algorithms characterized by different parameter values. Despite the large number of searchers tested in parallel, on various tasks this rather general approach compares favorably to a more specialized state-of-the-art heuristic; in one case it is nearly two orders of magnitude faster.},
    author = {Gagliolo, Matteo and Zhumatiy, Viktor and Schmidhuber, J\"{u}rgen},
    citeulike-article-id = {2270113},
    citeulike-linkout-0 = {http://www.springerlink.com/content/f71qqvx36cmufqwl},
    journal = {Machine Learning: ECML 2004},
    keywords = {ai, algorithm\_selection, machine\_learning, optimization, search, time\_allocation},
    pages = {134--143},
    posted-at = {2014-03-28 17:37:49},
    priority = {2},
    title = {Adaptive Online Time Allocation to Search Algorithms},
    url = {http://www.springerlink.com/content/f71qqvx36cmufqwl},
    year = {2004}
}

@incollection{Haim2009Restart,
    abstract = {Restart strategies are an important factor in the performance of conflict-driven Davis Putnam style SAT solvers. Selecting a good restart strategy for a problem instance can enhance the performance of a solver. Inspired by recent success applying machine learning techniques to predict the runtime of SAT solvers, we present a method which uses machine learning to boost solver performance through a smart selection of the restart strategy. Based on easy to compute features, we train both a satisfiability classifier and runtime models. We use these models to choose between restart strategies. We present experimental results comparing this technique with the most commonly used restart strategies. Our results demonstrate that machine learning is effective in improving solver performance.},
    author = {Haim, Shai and Walsh, Toby},
    citeulike-article-id = {5709776},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/978-3-642-02777-2\_30},
    citeulike-linkout-1 = {http://www.springerlink.com/content/u1346rl5u4025878},
    doi = {10.1007/978-3-642-02777-2\_30},
    journal = {Theory and Applications of Satisfiability Testing - SAT 2009},
    keywords = {algorithm\_selection, machine\_learning},
    pages = {312--325},
    posted-at = {2014-03-28 17:36:32},
    priority = {2},
    title = {Restart Strategy Selection Using Machine Learning Techniques},
    url = {http://dx.doi.org/10.1007/978-3-642-02777-2\_30},
    year = {2009}
}

@incollection{Nikolic2009InstanceBased,
    abstract = {Execution of most of the modern DPLL-based SAT solvers is guided by a number of heuristics. Decisions made during the search process are usually driven by some fixed heuristic policies. Despite the outstanding progress in SAT solving in recent years, there is still an appealing lack of techniques for selecting policies appropriate for solving specific input formulae. In this paper we present a methodology for instance-based selection of solver's policies that uses a data-mining classification technique. The methodology also relies on analysis of relationships between formulae, their families, and their suitable solving strategies. The evaluation results are very good, demonstrate practical usability of the methodology, and encourage further efforts in this direction.},
    author = {Nikoli\'{c}, Mladen and Mari\'{c}, Filip and Jani\v{c}i\'{c}, Predrag},
    citeulike-article-id = {5709929},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/978-3-642-02777-2\_31},
    citeulike-linkout-1 = {http://www.springerlink.com/content/uu2322r45t3vx116},
    doi = {10.1007/978-3-642-02777-2\_31},
    journal = {Theory and Applications of Satisfiability Testing - SAT 2009},
    keywords = {algorithm\_selection, machine\_learning},
    pages = {326--340},
    posted-at = {2014-03-28 17:35:01},
    priority = {2},
    title = {Instance-Based Selection of Policies for SAT Solvers},
    url = {http://dx.doi.org/10.1007/978-3-642-02777-2\_31},
    year = {2009}
}

@article{SmithMiles2011,
    abstract = {The suitability of an optimisation algorithm selected from within an algorithm portfolio depends upon the features of the particular instance to be solved. Understanding the relative strengths and weaknesses of different algorithms in the portfolio is crucial for effective performance prediction, automated algorithm selection, and to generate knowledge about the ideal conditions for each algorithm to influence better algorithm design. Relying on well-studied benchmark instances, or randomly generated instances, limits our ability to truly challenge each of the algorithms in a portfolio and determine these ideal conditions. Instead we use an evolutionary algorithm to evolve instances that are uniquely easy or hard for each algorithm, thus providing a more direct method for studying the relative strengths and weaknesses of each algorithm. The proposed methodology ensures that the meta-data is sufficient to be able to learn the features of the instances that uniquely characterise the ideal conditions for each algorithm. A case study is presented based on a comprehensive study of the performance of two heuristics on the Travelling Salesman Problem. The results show that prediction of search effort as well as the best performing algorithm for a given instance can be achieved with high accuracy.},
    author = {Smith-Miles, Kate and van Hemert, Jano},
    citeulike-article-id = {9219826},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/s10472-011-9230-5},
    citeulike-linkout-1 = {http://www.springerlink.com/content/6x83q3201gg71554},
    day = {19},
    doi = {10.1007/s10472-011-9230-5},
    issn = {1012-2443},
    journal = {Annals of Mathematics and Artificial Intelligence},
    keywords = {ai, algorithm\_selection, machine\_learning, metareasoning, optimisation},
    pages = {1--18},
    posted-at = {2014-03-28 17:34:03},
    priority = {2},
    publisher = {Springer Netherlands},
    title = {Discovering the suitability of optimisation algorithms by learning from evolved instances},
    url = {http://dx.doi.org/10.1007/s10472-011-9230-5},
    year = {2011}
}

@inproceedings{Fink1998How,
    abstract = {The choice of an appropriate problem-solving method, from

available methods, is a crucial skill for experts in many areas.

We describe a technique for the automatic selection among

methods, which is based on a statistical analysis of their past

performances.

We formalize the statistical problem involved in selecting

an efficient problem-solving method, derive a solution to this

problem, and describe a method-selection algorithm. The

algorithm not only chooses among available methods,...},
    author = {Fink, Eugene},
    booktitle = {Artificial Intelligence Planning Systems},
    citeulike-article-id = {2137842},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.7440},
    keywords = {ai, algorithm\_selection, machine\_learning, metareasoning, problem\_solving},
    pages = {128--136},
    posted-at = {2014-03-28 17:33:11},
    priority = {2},
    title = {How to Solve It Automatically: Selection Among Problem Solving Methods},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.7440},
    year = {1998}
}

@article{Gagliolo2011Algorithm,
    abstract = {We propose a method that learns to allocate computation time to a given set of algorithms, of unknown performance, with the aim of solving a given sequence of problem instances in a minimum time. Analogous meta-learning techniques are typically based on models of algorithm performance, learned during a separate offline training sequence, which can be prohibitively expensive. We adopt instead an online approach, named G AMBLE TA, in which algorithm performance models are iteratively updated, and used to guide allocation on a sequence of problem instances. G AMBLE TA is a general method for selecting among two or more alternative algorithm portfolios. Each portfolio has its own way of allocating computation time to the available algorithms, possibly based on performance models, in which case its performance is expected to improve over time, as more runtime data becomes available. The resulting exploration-exploitation trade-off is represented as a bandit problem. In our previous work, the algorithms corresponded to the arms of the bandit, and allocations evaluated by the different portfolios were mixed, using a solver for the bandit problem with expert advice, but this required the setting of an arbitrary bound on algorithm runtimes, invalidating the optimal regret of the solver. In this paper, we propose a simpler version of G AMBLE TA, in which the allocators correspond to the arms, such that a single portfolio is selected for each instance. The selection is represented as a bandit problem with partial information, and an unknown bound on losses. We devise a solver for this game, proving a bound on its expected regret. We present experiments based on results from several solver competitions, in various domains, comparing G AMBLE TA with another online method.},
    author = {Gagliolo, Matteo and Schmidhuber, J\"{u}rgen},
    citeulike-article-id = {9122579},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/s10472-011-9228-z},
    citeulike-linkout-1 = {http://www.springerlink.com/content/0h1262t3q7256011},
    day = {1},
    doi = {10.1007/s10472-011-9228-z},
    issn = {1012-2443},
    journal = {Annals of Mathematics and Artificial Intelligence},
    keywords = {ai, algorithm\_selection, machine\_learning, reinforcement\_learning},
    number = {2},
    pages = {49--86},
    posted-at = {2014-03-28 17:32:12},
    priority = {2},
    publisher = {Springer Netherlands},
    title = {Algorithm portfolio selection as a bandit problem with unbounded losses},
    url = {http://dx.doi.org/10.1007/s10472-011-9228-z},
    volume = {61},
    year = {2011}
}

@article{Gagliolo2006Learning,
    abstract = {Algorithm selection can be performed using a model of runtime distribution, learned during a preliminary training phase. There is a trade-off between the performance of model-based algorithm selection, and the cost of learning the model. In this paper, we treat this trade-off in the context of bandit problems. We propose a fully dynamic and online algorithm selection technique, with no separate training phase: all candidate algorithms are run in parallel, while a model incrementally learns their runtime distributions. A redundant set of time allocators uses the partially trained model to propose machine time shares for the algorithms. A bandit problem solver mixes the model-based shares with a uniform share, gradually increasing the impact of the best time allocators as the model improves. We present experiments with a set of SAT solvers on a mixed SAT-UNSAT benchmark; and with a set of solvers for the Auction Winner Determination problem.},
    author = {Gagliolo, Matteo and Schmidhuber, Jurgen},
    citeulike-article-id = {1168556},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1227618.1227636},
    citeulike-linkout-1 = {http://dx.doi.org/10.1007/s10472-006-9036-z},
    citeulike-linkout-2 = {http://www.ingentaconnect.com/content/klu/amai/2006/00000047/F0020003/00009036},
    doi = {10.1007/s10472-006-9036-z},
    issn = {1012-2443},
    journal = {Annals of Mathematics and Artificial Intelligence},
    keywords = {ai, algorithm\_selection, machine\_learning},
    number = {3-4},
    pages = {295--328},
    posted-at = {2014-03-28 17:31:27},
    priority = {2},
    publisher = {Springer},
    title = {Learning dynamic algorithm portfolios},
    url = {http://dx.doi.org/10.1007/s10472-006-9036-z},
    volume = {47},
    year = {2006}
}

@inproceedings{Mccracken2003Performance,
    author = {Mccracken, Michael O. and Snavely, Allan and Malony, Allen D.},
    booktitle = {International Conference on Computational Science},
    citeulike-article-id = {2116868},
    citeulike-linkout-0 = {http://dblp.uni-trier.de/rec/bibtex/conf/iccS/McCrackenSM03},
    editor = {Sloot, Peter M. A. and Abramson, David and Bogdanov, Alexander V. and Dongarra, Jack and Zomaya, Albert Y. and Gorbachev, Yuri E. and Sloot, Peter M. A. and Abramson, David and Bogdanov, Alexander V. and Dongarra, Jack and Zomaya, Albert Y. and Gorbachev, Yuri E.},
    keywords = {ai, algorithm\_selection, metareasoning},
    pages = {749--758},
    posted-at = {2014-03-28 17:30:11},
    priority = {2},
    publisher = {Springer},
    series = {Lecture Notes in Computer Science},
    title = {Performance Modeling for Dynamic Algorithm Selection},
    url = {http://dblp.uni-trier.de/rec/bibtex/conf/iccS/McCrackenSM03},
    volume = {2660},
    year = {2003}
}

@misc{BrownPortfolio,
    abstract = {this paper describes a technique for
using rejection sampling to automatically generate such instances.
In Figures 4 and 5 we show how our techniques are
able to automatically skew two of the easiest CATS instance
distributions towards harder regions. In fact, for these two
distributions we generated instances that were (respectively)
100 and 50 times harder than anything we had previously
seen! Moreover, the average runtime for the new distributions
was greater than the observed maximum...},
    author = {Brown, Kevin L. and Nudelman, Eugene and Andrew, Galen and Mcfadden, Jim and Shoham, Yoav},
    citeulike-article-id = {2004146},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.12.7695},
    keywords = {ai, algorithm\_selection},
    posted-at = {2014-03-28 17:29:31},
    priority = {2},
    title = {A Portfolio Approach to Algorithm Selection},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.12.7695}
}

@inproceedings{Yu2004Adaptive,
    abstract = {Irregular and dynamic memory reference patterns can cause performance variations for low level algorithms in general and for parallel algorithms in particular. We present an adaptive algorithm selection framework which can collect and interpret the inputs of a particular instance of a parallel algorithm and select the best performing one from a an existing library. In this paper present the dynamic selection of parallel reduction algorithms. First we introduce a set of high-level parameters that can characterize different parallel reduction algorithms. Then we describe an off-line, systematic process to generate predictive models which can be used for run-time algorithm selection. Our experiments show that our framework: (a) selects the most appropriate algorithms in 85\% of the cases studied, (b) overall delievers 98\% of the optimal performance, (c) adaptively selects the best algorithms for dynamic phases of a running program (resulting in performance improvements otherwise not possible), and (d) adapts to the underlying machine architecture (tested on IBM Regatta and HP V-Class systems).},
    address = {Washington, DC, USA},
    author = {Yu, Hao and Zhang, Dongmin and Rauchwerger, Lawrence},
    booktitle = {Proceedings of the 13th International Conference on Parallel Architectures and Compilation Techniques},
    citeulike-article-id = {2116958},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1026016},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/pact.2004.6},
    doi = {10.1109/pact.2004.6},
    isbn = {0-7695-2229-7},
    keywords = {ai, algorithm\_selection, machine\_learning},
    pages = {278--289},
    posted-at = {2014-03-28 17:28:50},
    priority = {2},
    publisher = {IEEE Computer Society},
    series = {PACT '04},
    title = {An Adaptive Algorithm Selection Framework},
    url = {http://dx.doi.org/10.1109/pact.2004.6},
    year = {2004}
}

@electronic{Fagg2006Decision,
    abstract = {Selecting the close-to-optimal collective algorithm based on the parameters of the collective call at run time is an important step in achieving good performance of MPI applications. In this paper, we explore the applicability of C4.5 decision trees to the MPI collective algorithm selection problem. We construct C4.5 decision trees from the measured algorithm performance data and analyze the decision tree properties and expected run time performance penalty. In cases we considered, results show that the C4.5 decision trees can be used to generate a reasonably small and very accurate decision function. For example, the Broadcast decision tree with only 21 leaves was able to achieve a mean performance penalty of 2.08\%. Similarly, combining experimental data for Reduce and Broadcast and generating a decision function from the combined decision trees resulted in less than 2.5 \% relative performance penalty. The results indicate that C4.5 decision trees are applicable to this problem and should be more widely used in this domain. 1},
    author = {Fagg, Graham E. and Angskun, Thara and Bosilca, George and Dongarra, Jack J.},
    citeulike-article-id = {3177107},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.101.2641},
    keywords = {ai, algorithm\_selection},
    posted-at = {2014-03-28 17:28:01},
    priority = {2},
    title = {Decision trees and MPI collective algorithm selection problem},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.101.2641},
    volume = {2006},
    year = {2006}
}

@article{RamakrishnanGAUSS,
    abstract = {We describe the design and implementation of GAUSS - an online algorithm selection system for numerical quadrature. Given a quadrature problem and performance constraints on its solution, GAUSS selects the best (or nearly best) algorithm. GAUSS uses inductive logic programming to generalize a database of performance data; this produces high-level rules that correlate problem features with algorithm performance. Such rules then serve as the basis for recommending algorithms for new problem instances. GAUSS functions online (new data and information can be incrementally incorporated) and can also provide phenomenological explanations of algorithm recommendations.},
    author = {Ramakrishnan, N.},
    citeulike-article-id = {1350897},
    citeulike-linkout-0 = {http://www.ingentaconnect.com/content/els/09659978/2002/00000033/00000001/art00046},
    keywords = {ai, algorithm\_selection},
    pages = {27--36},
    posted-at = {2014-03-28 17:27:28},
    priority = {2},
    title = {GAUSS: an online algorithm selection system for numerical quadrature},
    url = {http://www.ingentaconnect.com/content/els/09659978/2002/00000033/00000001/art00046}
}

@proceedings{Yu2004Adaptive,
    abstract = {Irregular and dynamic memory reference patterns can cause performance variations for low level algorithms in general and for parallel algorithms in particular. We present an adaptive algorithm selection framework which can collect and interpret the inputs of a particular instance of a parallel algorithm and select the best performing one from an existing library. We present the dynamic selection of parallel reduction algorithms. First we introduce a set of high-level parameters that can characterize different parallel reduction algorithms. Then we describe an offline, systematic process to generate predictive models, which can be used for run-time algorithm selection. Our experiments show that our framework: (a) selects the most appropriate algorithms in 85\% of the cases studied, (b) overall delivers 98\% of the optimal performance, (c) adaptively selects the best algorithms for dynamic phases of a running program (resulting in performance improvements otherwise not possible), and (d) adapts to the underlying machine architecture (tested on IBM Regatta and HP V-Class systems).},
    author = {Yu, H. and Zhang, D. and Rauchwerger, L.},
    booktitle = {Parallel Architecture and Compilation Techniques, 2004. PACT 2004. Proceedings. 13th International Conference on},
    citeulike-article-id = {3066589},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/pact.2004.1342561},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1342561},
    doi = {10.1109/pact.2004.1342561},
    journal = {Parallel Architecture and Compilation Techniques, 2004. PACT 2004. Proceedings. 13th International Conference on},
    keywords = {ai, algorithm\_selection, machine\_learning},
    pages = {278--289},
    posted-at = {2014-03-28 17:27:00},
    priority = {2},
    title = {An adaptive algorithm selection framework},
    url = {http://dx.doi.org/10.1109/pact.2004.1342561},
    year = {2004}
}

@article{Pirkelbauer2009Dynamic,
    abstract = {A key benefit of generic programming is its support for producing modules with clean separation. In particular, generic algorithms are written to work with a wide variety of types without requiring modifications to them. The  Runtime concept  idiom extends this support by allowing unmodified concrete types to behave in a runtime polymorphic manner. In this paper, we describe one implementation of the runtime concept idiom, in the domain of the C++ standard template library (STL). We complement the runtime concept idiom with an algorithm library that considers both type and concept information to maximize performance when selecting algorithm implementations. We present two implementations, one in ISO C++ and one using an experimental language extension. We use our implementations to describe and measure the performance of runtime-polymorphic analogs of several STL algorithms. The tests demonstrate the effects of different compile-time vs. run-time algorithm selection choices.},
    author = {Pirkelbauer, Peter and Parent, Sean and Marcus, Mat and Stroustrup, Bjarne},
    citeulike-article-id = {5333757},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.scico.2009.04.002},
    day = {13},
    doi = {10.1016/j.scico.2009.04.002},
    issn = {01676423},
    journal = {Science of Computer Programming},
    keywords = {algorithm\_selection},
    posted-at = {2014-03-28 17:25:51},
    priority = {2},
    title = {Dynamic algorithm selection for runtime concepts},
    url = {http://dx.doi.org/10.1016/j.scico.2009.04.002},
    year = {2009}
}

@inproceedings{Hutter2006Performance,
    abstract = {Machine learning can be used to build models that predict the run-time of search algorithms for hard combinatorial problems. Such empirical hardness models have previously been studied for complete, deterministic search algorithms. In this work, we demonstrate that such models can also make surprisingly accurate predictions of the run-time distributions of incomplete and randomized search methods, such as stochastic local search algorithms. We also show for the first time how information about an algorithm's parameter settings can be incorporated into a model, and how such models can be used to automatically adjust the algorithm's parameters on a per-instance basis in order to optimize its performance. Empirical results for Novelty+ and SAPS on structured and unstructured SAT instances show very good predictive performance and significant speedups of our automatically determined parameter settings when compared to the default and best fixed distribution-specific parameter settings.},
    author = {Hutter, Frank and Hamadi, Youssef and Hoos, Holger H. and Leyton-Brown, Kevin},
    booktitle = {Principles and Practice of Constraint Programming (CP)},
    citeulike-article-id = {2711112},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/11889205\_17},
    doi = {10.1007/11889205\_17},
    journal = {Principles and Practice of Constraint Programming - CP 2006},
    keywords = {ai, algorithm\_selection, machine\_learning, metareasoning, search},
    pages = {213--228},
    posted-at = {2014-03-28 17:24:58},
    priority = {2},
    title = {{Performance Prediction and Automated Tuning of Randomized and Parametric Algorithms}},
    url = {http://dx.doi.org/10.1007/11889205\_17},
    year = {2006}
}

@article{Hutter2010Tradeoffs,
    abstract = {We propose an empirical analysis approach for characterizing tradeoffs between different methods for comparing a set of competing algorithm designs. Our approach can provide insight into performance variation both across candidate algorithms and across instances. It can also identify the best tradeoff between evaluating a larger number of candidate algorithm designs, performing these evaluations on a larger number of problem instances, and allocating more time to each algorithm run. We applied our approach to a study of the rich algorithm design spaces offered by three highly-parameterized, state-of-the-art algorithms for satisfiability and mixed integer programming, considering six different distributions of problem instances. We demonstrate that the resulting algorithm design scenarios differ in many ways, with important consequences for both automatic and manual algorithm design. We expect that both our methods and our findings will lead to tangible improvements in algorithm design methods.},
    address = {Hingham, MA, USA},
    author = {Hutter, Frank and Hoos, Holger H. and Brown, Kevin L.},
    citeulike-article-id = {7376447},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=2004383},
    citeulike-linkout-1 = {http://dx.doi.org/10.1007/s10472-010-9191-0},
    citeulike-linkout-2 = {http://www.springerlink.com/content/865w5107127h70x3},
    day = {10},
    doi = {10.1007/s10472-010-9191-0},
    issn = {1012-2443},
    journal = {Annals of Mathematics and Artificial Intelligence},
    keywords = {ai, algorithm\_selection, machine\_learning},
    pages = {65--89},
    posted-at = {2014-03-28 17:24:05},
    priority = {2},
    publisher = {Kluwer Academic Publishers},
    title = {Tradeoffs in the empirical evaluation of competing algorithm designs},
    url = {http://dx.doi.org/10.1007/s10472-010-9191-0},
    volume = {60},
    year = {2010}
}

@inproceedings{Lagoudakis2000Algorithm,
    address = {San Francisco, CA, USA},
    author = {Lagoudakis, Michail G. and Littman, Michael L.},
    booktitle = {ICML '00: Proceedings of the Seventeenth International Conference on Machine Learning},
    citeulike-article-id = {2270071},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=657981},
    isbn = {1558607072},
    keywords = {ai, algorithm\_selection, machine\_learning, metareasoning, reinforcement\_learning, strategy\_selection},
    pages = {511--518},
    posted-at = {2014-03-28 17:17:49},
    priority = {2},
    publisher = {Morgan Kaufmann Publishers Inc.},
    title = {Algorithm Selection using Reinforcement Learning},
    url = {http://portal.acm.org/citation.cfm?id=657981},
    year = {2000}
}

@article{LeFevre1996Selection,
    author = {LeFevre, Jo-Anne and Sadesky, Gregory S. and Bisanz, Jeffrey},
    citeulike-article-id = {13113944},
    citeulike-linkout-0 = {http://dx.doi.org/10.1037/0278-7393.22.1.216},
    doi = {10.1037/0278-7393.22.1.216},
    issn = {0278-7393},
    journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
    keywords = {algorithm\_selection, cognition, mental\_arithmetic, metacognition, strategy\_selection},
    number = {1},
    pages = {216--230},
    posted-at = {2014-03-21 20:47:45},
    priority = {2},
    title = {Selection of procedures in mental addition: Reassessing the problem size effect in adults.},
    url = {http://dx.doi.org/10.1037/0278-7393.22.1.216},
    volume = {22},
    year = {1996}
}

@article{LeFevre2006Selection,
    author = {LeFevre, Jo-Anne and DeStefano, Diana and Penner-Wilger, Marcie and Daley, Karen E.},
    citeulike-article-id = {13113942},
    citeulike-linkout-0 = {http://dx.doi.org/10.1037/cjep2006020},
    doi = {10.1037/cjep2006020},
    issn = {1878-7290},
    journal = {Canadian Journal of Experimental Psychology/Revue canadienne de psychologie exp\'{e}rimentale},
    keywords = {algorithm\_selection, cognition, mental\_arithmetic, metacognition, strategy\_selection},
    number = {3},
    pages = {209--220},
    posted-at = {2014-03-21 20:46:50},
    priority = {2},
    title = {Selection of procedures in mental subtraction.},
    url = {http://dx.doi.org/10.1037/cjep2006020},
    volume = {60},
    year = {2006}
}

@article{Vasilikos2010Optimization,
    abstract = {The traditional approach to computational problem solving is to use one of the available algorithms to obtain solutions for all given instances of a problem. However, typically not all instances are the same, nor a single algorithm performs best on all instances. Our work investigates a more sophisticated approach to problem solving, called Recursive Algorithm Selection, whereby several algorithms for a problem (including some recursive ones) are available to an agent that makes an informed decision on which algorithm to select for handling each sub-instance of a problem at each recursive call made while solving an instance. Reinforcement learning methods are used for learning decision policies that optimize any given performance criterion (time, memory, or a combination thereof) from actual execution and profiling experience. This paper focuses on the well-known problem of state-space heuristic search and combines the A* and RBFS algorithms to yield a hybrid search algorithm, whose decision policy is learned using the Least-Squares Policy Iteration (LSPI) algorithm. Our benchmark problem domain involves shortest path finding problems in a real-world dataset encoding the entire street network of the District of Columbia (DC), USA. The derived hybrid algorithm exhibits better performance results than the individual algorithms in the majority of cases according to a variety of performance criteria balancing time and memory. It is noted that the proposed methodology is generic, can be applied to a variety of other problems, and requires no prior knowledge about the individual algorithms used or the properties of the underlying problem instances being solved.},
    author = {Vasilikos, Vasileios and Lagoudakis, MichailG},
    booktitle = {Annals of Mathematics and Artificial Intelligence},
    citeulike-article-id = {8423336},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/s10472-010-9217-7},
    citeulike-linkout-1 = {http://www.springerlink.com/content/j8p1x127590wt607},
    citeulike-linkout-2 = {http://link.springer.com/article/10.1007/s10472-010-9217-7},
    day = {1},
    doi = {10.1007/s10472-010-9217-7},
    issn = {1012-2443},
    journal = {Annals of Mathematics and Artificial Intelligence},
    keywords = {ai, algorithm\_selection, machine\_learning, metacognition, metalearning, metareasoning, reinforcement\_learning, search},
    number = {1-2},
    pages = {119--151},
    posted-at = {2014-03-21 08:16:47},
    priority = {2},
    publisher = {Springer Netherlands},
    title = {Optimization of heuristic search using recursive algorithm selection and reinforcement learning},
    url = {http://dx.doi.org/10.1007/s10472-010-9217-7},
    volume = {60},
    year = {2010}
}

@inproceedings{bognar2008method,
    author = {Bognar, Carlos E. and Saotome, Osamu and Mastorakis, N. E. and Mladenov, V. and Bojkovic, Z. and Simian, D. and Kartalopoulos, S. and Varonides, A. and Udriste, C. and Kindler, E. and Others},
    booktitle = {WSEAS International Conference. Proceedings. Mathematics and Computers in Science and Engineering},
    citeulike-article-id = {13113295},
    keywords = {ai, algorithm\_selection, bayesian, bayesian\_inference, inference, machine\_learning, metacognition, metalearning, metareasoning, strategy\_selection},
    number = {12},
    organization = {WSEAS},
    posted-at = {2014-03-21 08:06:03},
    priority = {3},
    title = {A method for Bayesian meta-inference applying multiple regressions},
    year = {2008}
}

@article{Wolpert1997,
  title={No free lunch theorems for optimization},
  author={Wolpert, David H and Macready, William G},
  journal={Evolutionary Computation, IEEE Transactions on},
  volume={1},
  number={1},
  pages={67--82},
  year={1997},
  publisher={IEEE}
}
@inproceedings{Harada1998,
    author = {Harada, D. and Russell, S.},
    booktitle = {NIPS'98 Workshop on Abstraction and Hierarchy in Reinforcement Learning},
    citeulike-article-id = {2508490},
    keywords = {metacognition, reinforcement\_learning},
    posted-at = {2013-10-11 03:59:10},
    priority = {2},
    title = {Meta-Level Reinforcement Learning},
    year = {1998}
}
@article{Kotthoff2014,
        title = {Algorithm Selection for Combinatorial Search Problems: A Survey},
        abstract = {The Algorithm Selection Problem is concerned with selecting the best algorithm
to solve a given problem instance on a case-by-case basis. It has become
especially relevant in the last decade, with researchers increasingly
investigating how to identify the most suitable existing algorithm for solving a
problem instance instead of developing new algorithms. This survey presents an
overview of this work focusing on the contributions made in the area of
combinatorial search problems, where algorithm selection techniques have
achieved significant performance improvements. We unify and organise the vast
literature according to criteria that determine algorithm selection systems in
practice. The comprehensive classification of approaches identifies and analyses
the different directions from which algorithm selection has been approached.
This paper contrasts and compares different methods for solving the problem as
well as ways of using these solutions.},
        journal = {{AI} Magazine},
        author = {Kotthoff, Lars},
        year = {2014}
}
@incollection{Guo2005,
    abstract = {The algorithm selection problem aims to select the best algorithm for an input problem instance according to some characteristics of the instance. This paper presents a learning-based inductive approach to build a predictive algorithm selection system from empirical algorithm performance data of the Most Probable Explanation(MPE) problem. The learned model can serve as an algorithm selection meta-reasoner for the real-time MPE problem. Experimental results show that the learned algorithm selection models can help integrate multiple MPE algorithms to gain a better overall performance of reasoning.},
    author = {Guo, Haipeng and Hsu, WilliamH},
    booktitle = {AI 2004: Advances in Artificial Intelligence},
    citeulike-article-id = {13113292},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/978-3-540-30549-1\_28},
    citeulike-linkout-1 = {http://link.springer.com/chapter/10.1007/978-3-540-30549-1\_28},
    doi = {10.1007/978-3-540-30549-1\_28},
    editor = {Webb, GeoffreyI and Yu, Xinghuo},
    keywords = {ai, algorithm\_selection, bayesian, bayesian\_inference, inference, machine\_learning, metacognition, metalearning, metareasoning, real-time-systems},
    pages = {307--318},
    posted-at = {2014-03-21 07:59:42},
    priority = {2},
    publisher = {Springer Berlin Heidelberg},
    series = {Lecture Notes in Computer Science},
    title = {A Learning-Based Algorithm Selection Meta-Reasoner for the Real-Time MPE Problem},
    url = {http://dx.doi.org/10.1007/978-3-540-30549-1\_28},
    volume = {3339},
    year = {2005}
}

@article{Guo2007,
    abstract = {Given one instance of an 
              
                                
                                \$\mathcal{NP}\$
                              
              -hard optimization problem, can we tell in advance whether it is exactly solvable or not? If it is not, can we predict which approximate algorithm is the best to solve it? Since the behavior of most approximate, randomized, and heuristic search algorithms for 
              
                                
                                \$\mathcal{NP}\$
                              
              -hard problems is usually very difficult to characterize analytically, researchers have turned to experimental methods in order to answer these questions. In this paper we present a machine learning-based approach to address the above questions. Models induced from algorithmic performance data can represent the knowledge of how algorithmic performance depends on some easy-to-compute problem instance characteristics. Using these models, we can estimate approximately whether an input instance is exactly solvable or not. Furthermore, when it is classified as exactly unsolvable, we can select the best approximate algorithm for it among a list of candidates. In this paper we use the MPE (most probable explanation) problem in probabilistic inference as a case study to validate the proposed methodology. Our experimental results show that the machine learning-based algorithm selection system can integrate both exact and inexact algorithms and provide the best overall performance comparing to any single candidate algorithm.},
    author = {Guo, Haipeng and Hsu, WilliamH},
    booktitle = {Annals of Operations Research},
    citeulike-article-id = {5382029},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/s10479-007-0229-6},
    citeulike-linkout-1 = {http://www.springerlink.com/content/62l1816582836672},
    citeulike-linkout-2 = {http://link.springer.com/article/10.1007/s10479-007-0229-6},
    day = {1},
    doi = {10.1007/s10479-007-0229-6},
    journal = {Annals of Operations Research},
    keywords = {ai, algorithm\_selection, bayesian, inference, machine\_learning, metacognition, metalearning, metareasoning},
    number = {1},
    pages = {61--82},
    posted-at = {2014-03-21 07:55:44},
    priority = {2},
    publisher = {Springer US},
    title = {A machine learning approach to algorithm selection for \mathcal{NP} -hard optimization problems: a case study on the MPE problem},
    url = {http://dx.doi.org/10.1007/s10479-007-0229-6},
    volume = {156},
    year = {2007}
}

@article{Smith2008,
    address = {New York, NY, USA},
    author = {Smith-Miles, Kate A.},
    citeulike-article-id = {13113289},
    citeulike-linkout-0 = {http://dx.doi.org/10.1145/1456650.1456656},
    citeulike-linkout-1 = {http://doi.acm.org/10.1145/1456650.1456656},
    doi = {10.1145/1456650.1456656},
    journal = {ACM Comput. Surv.},
    keywords = {ai, algorithm\_selection, machine\_learning, meta-learning, metacognition, metalearning, metareasoning, strategy\_selection},
    month = jan,
    number = {1},
    posted-at = {2014-03-21 07:51:58},
    priority = {4},
    publisher = {ACM},
    title = {Cross-disciplinary Perspectives on Meta-learning for Algorithm Selection},
    url = {http://doi.acm.org/10.1145/1456650.1456656},
    volume = {41},
    year = {2009}
}

@phdthesis{Guo2003,
    author = {Guo, Haipeng},
    citeulike-article-id = {13113288},
    keywords = {ai, algorithm\_selection, bayesian, inference, machine\_learning, metacognition, metareasoning, sorting, strategy\_selection},
    posted-at = {2014-03-21 07:48:11},
    priority = {4},
    school = {Kansas State University},
    title = {Algorithm selection for sorting and probabilistic inference: a machine learning-based approach},
    year = {2003}
}

@article{Colreavy2008Strategy,
    abstract = {The processes that determine unsupervised categorization, the task of classifying stimuli without guidance or feedback, are poorly understood. Two experiments examined the emergence and plasticity of unsupervised strategies using perceptual stimuli that varied along two separable dimensions. In the first experiment, participants either classified stimuli into any two categories of their choice or learned identical classifications by supervised categorization. Irrespective of the complexity of classification, supervised and unsupervised learning rates differed little when both modes of learning were maximally comparable. The second experiment examined the plasticity of unsupervised classifications by introducing novel stimuli halfway through training. Whether or not people altered their strategies, they responded to novel stimuli in a gradual manner. The gradual and continuous evolution and adaptation of strategies suggests that unsupervised categorization involves true learning which shares many properties of supervised category learning. We also show that the choice of unsupervised strategy cannot be predicted from the properties of early learning trials, but is best understood as a function of the initial distribution of dimensional attention.},
    author = {Colreavy, Erin and Lewandowsky, Stephan},
    booktitle = {Memory \& Cognition},
    citeulike-article-id = {2845349},
    citeulike-linkout-0 = {http://dx.doi.org/10.3758/mc.36.4.762},
    citeulike-linkout-1 = {http://www.ingentaconnect.com/content/psocpubs/mrc/2008/00000036/00000004/art00006},
    citeulike-linkout-2 = {http://link.springer.com/article/10.3758/MC.36.4.762},
    doi = {10.3758/mc.36.4.762},
    issn = {0090-502X},
    journal = {Memory \&\#38; Cognition},
    keywords = {categorization, cognition, learning, strategy\_selection, strategy\_selection\_learning},
    month = jun,
    number = {4},
    pages = {762--775},
    posted-at = {2014-05-05 02:53:21},
    priority = {2},
    publisher = {Springer-Verlag},
    title = {Strategy development and learning differences in supervised and unsupervised categorization},
    url = {http://dx.doi.org/10.3758/mc.36.4.762},
    volume = {36},
    year = {2008}
}

@inproceedings{Lagoudakis2000Algorithm,
    address = {San Francisco, CA, USA},
    author = {Lagoudakis, Michail G. and Littman, Michael L.},
    booktitle = {ICML '00: Proceedings of the Seventeenth International Conference on Machine Learning},
    citeulike-article-id = {2270071},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=657981},
    isbn = {1558607072},
    keywords = {ai, algorithm\_selection, machine\_learning, metareasoning, reinforcement\_learning, strategy\_selection},
    pages = {511--518},
    posted-at = {2014-03-28 17:17:49},
    priority = {2},
    publisher = {Morgan Kaufmann Publishers Inc.},
    title = {Algorithm Selection using Reinforcement Learning},
    url = {http://portal.acm.org/citation.cfm?id=657981},
    year = {2000}
}

@article{Fulvio2014TaskSpecific,
    abstract = {The goal of training is to produce learning for a range of activities that are typically more general than the training task itself. Despite a century of research, predicting the scope of learning from the content of training has proven extremely difficult, with the same task producing narrowly focused learning strategies in some cases and broadly scoped learning strategies in others. Here we test the hypothesis that human subjects will prefer a decision strategy that maximizes performance and reduces uncertainty given the demands of the training task and that the strategy chosen will then predict the extent to which learning is transferable. To test this hypothesis, we trained subjects on a moving dot extrapolation task that makes distinct predictions for two types of learning strategy: a narrow model-free strategy that learns an input-output mapping for training stimuli, and a general model-based strategy that utilizes humans' default predictive model for a class of trajectories. When the number of distinct training trajectories is low, we predict better performance for the mapping strategy, but as the number increases, a predictive model is increasingly favored. Consonant with predictions, subject extrapolations for test trajectories were consistent with using a mapping strategy when trained on a small number of training trajectories and a predictive model when trained on a larger number. The general framework developed here can thus be useful both in interpreting previous patterns of task-specific versus task-general learning, as well as in building future training paradigms with certain desired outcomes. Predicting what humans will learn from a training task, in particular, whether learning will generalize beyond the specifics of the given experience, is of both significant practical and theoretical interest. However, a principled understanding of the relationship between training conditions and learning generalization remains elusive. In this paper, we develop a computational framework for predicting which of two basic decision-making strategies will be utilized by human subjects - 1) simple stimulus-response mappings or 2) predictive models. Through simulation, we show that the nature of the training experience determines which of these categories leads to better in-task performance; repetitive training on a small set of examples favors simple stimulus-response mappings, whereas training on a large set of examples favors predictive strategies. We then show that humans trained under these various conditions do indeed utilize the predicted strategy. Finally, we show that the strategies that are utilized during training predict generalization of learning. Those who learn simple mappings fail to generalize their new skills, in contrast to those who use default predictive strategies. The framework developed here is useful both in interpreting previous patterns of learning, as well as in building training paradigms with given desired outcomes.},
    author = {Fulvio, Jacqueline M. and Green, C. Shawn and Schrater, Paul R.},
    citeulike-article-id = {13118579},
    citeulike-linkout-0 = {http://dx.doi.org/10.1371/journal.pcbi.1003425},
    day = {2},
    doi = {10.1371/journal.pcbi.1003425},
    journal = {PLoS Comput Biol},
    keywords = {cognition, cognitive\_training, generalization, learning, strategy\_selection},
    month = jan,
    number = {1},
    pages = {e1003425+},
    posted-at = {2014-03-27 20:45:30},
    priority = {2},
    publisher = {Public Library of Science},
    title = {Task-Specific Response Strategy Selection on the Basis of Recent Training Experience},
    url = {http://dx.doi.org/10.1371/journal.pcbi.1003425},
    volume = {10},
    year = {2014}
}

@article{Holmes2014Optimality,
    abstract = {We review how leaky competing accumulators (LCAs) can be used to model decision making in two-alternative, forced-choice tasks, and we show how they reduce to drift diffusion (DD) processes in special cases. As continuum limits of the sequential probability ratio test, DD processes are optimal in producing decisions of specified accuracy in the shortest possible time. Furthermore, the DD model can be used to derive a speed–accuracy trade-off that optimizes reward rate for a restricted class of two alternative forced-choice decision tasks. We review findings that compare human performance with this benchmark, and we reveal both approximations to and deviations from optimality. We then discuss three potential sources of deviations from optimality at the psychological level—avoidance of errors, poor time estimation, and minimization of the cost of control—and review recent theoretical and empirical findings that address these possibilities. We also discuss the role of cognitive control in changing environments and in modulating exploitation and exploration. Finally, we consider physiological factors in which nonlinear dynamics may also contribute to deviations from optimality.},
    author = {Holmes, Philip and Cohen, Jonathan D.},
    citeulike-article-id = {13118515},
    citeulike-linkout-0 = {http://dx.doi.org/10.1111/tops.12084},
    day = {1},
    doi = {10.1111/tops.12084},
    journal = {Top Cogn Sci},
    keywords = {adaptation, cognition, cognitive\_control, computational\_cognitive\_science, decision\_making, exploration-exploitation, metacognition, rationality, resource-rationality, speed-accuracy\_tradeoff, strategy\_selection},
    month = mar,
    pages = {n/a},
    posted-at = {2014-03-27 19:34:27},
    priority = {2},
    title = {Optimality and Some of Its Discontents: Successes and Shortcomings of Existing Models for Binary Decisions},
    url = {http://dx.doi.org/10.1111/tops.12084},
    year = {2014}
}

@article{LeFevre1996Selection,
    author = {LeFevre, Jo-Anne and Sadesky, Gregory S. and Bisanz, Jeffrey},
    citeulike-article-id = {13113944},
    citeulike-linkout-0 = {http://dx.doi.org/10.1037/0278-7393.22.1.216},
    doi = {10.1037/0278-7393.22.1.216},
    issn = {0278-7393},
    journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
    keywords = {algorithm\_selection, cognition, mental\_arithmetic, metacognition, strategy\_selection},
    number = {1},
    pages = {216--230},
    posted-at = {2014-03-21 20:47:45},
    priority = {2},
    title = {Selection of procedures in mental addition: Reassessing the problem size effect in adults.},
    url = {http://dx.doi.org/10.1037/0278-7393.22.1.216},
    volume = {22},
    year = {1996}
}

@article{LeFevre2006Selection,
    author = {LeFevre, Jo-Anne and DeStefano, Diana and Penner-Wilger, Marcie and Daley, Karen E.},
    citeulike-article-id = {13113942},
    citeulike-linkout-0 = {http://dx.doi.org/10.1037/cjep2006020},
    doi = {10.1037/cjep2006020},
    issn = {1878-7290},
    journal = {Canadian Journal of Experimental Psychology/Revue canadienne de psychologie exp\'{e}rimentale},
    keywords = {algorithm\_selection, cognition, mental\_arithmetic, metacognition, strategy\_selection},
    number = {3},
    pages = {209--220},
    posted-at = {2014-03-21 20:46:50},
    priority = {2},
    title = {Selection of procedures in mental subtraction.},
    url = {http://dx.doi.org/10.1037/cjep2006020},
    volume = {60},
    year = {2006}
}

@inproceedings{bognar2008method,
    author = {Bognar, Carlos E. and Saotome, Osamu and Mastorakis, N. E. and Mladenov, V. and Bojkovic, Z. and Simian, D. and Kartalopoulos, S. and Varonides, A. and Udriste, C. and Kindler, E. and Others},
    booktitle = {WSEAS International Conference. Proceedings. Mathematics and Computers in Science and Engineering},
    citeulike-article-id = {13113295},
    keywords = {ai, algorithm\_selection, bayesian, bayesian\_inference, inference, machine\_learning, metacognition, metalearning, metareasoning, strategy\_selection},
    number = {12},
    organization = {WSEAS},
    posted-at = {2014-03-21 08:06:03},
    priority = {3},
    title = {A method for Bayesian meta-inference applying multiple regressions},
    year = {2008}
}

@misc{Kotthoff2012Algorithm,
    abstract = {The Algorithm Selection Problem is concerned with selecting the best
algorithm to solve a given problem on a case-by-case basis. It has become
especially relevant in the last decade, as researchers are increasingly
investigating how to identify the most suitable existing algorithm for solving
a problem instead of developing new algorithms. This survey presents an
overview of this work focusing on the contributions made in the area of
combinatorial search problems, where Algorithm Selection techniques have
achieved significant performance improvements. We unify and organise the vast
literature according to criteria that determine Algorithm Selection systems in
practice. The comprehensive classification of approaches identifies and
analyses the different directions from which Algorithm Selection has been
approached. This paper contrasts and compares different methods for solving the
problem as well as ways of using these solutions. It closes by identifying
directions of current and future research.},
    archivePrefix = {arXiv},
    author = {Kotthoff, Lars},
    citeulike-article-id = {11607814},
    citeulike-linkout-0 = {http://arxiv.org/abs/1210.7959},
    citeulike-linkout-1 = {http://arxiv.org/pdf/1210.7959},
    day = {30},
    eprint = {1210.7959},
    keywords = {ai, algorithm\_selection, metacognition, metalearning, metareasoning, search, strategy\_selection},
    month = oct,
    posted-at = {2014-03-21 08:00:58},
    priority = {2},
    title = {Algorithm Selection for Combinatorial Search Problems: A Survey},
    url = {http://arxiv.org/abs/1210.7959},
    year = {2012}
}

@article{Smith2008,
    address = {New York, NY, USA},
    author = {Smith-Miles, Kate A.},
    citeulike-article-id = {13113289},
    citeulike-linkout-0 = {http://dx.doi.org/10.1145/1456650.1456656},
    citeulike-linkout-1 = {http://doi.acm.org/10.1145/1456650.1456656},
    doi = {10.1145/1456650.1456656},
    journal = {ACM Comput. Surv.},
    keywords = {ai, algorithm\_selection, machine\_learning, meta-learning, metacognition, metalearning, metareasoning, strategy\_selection},
    month = jan,
    number = {1},
    posted-at = {2014-03-21 07:51:58},
    priority = {4},
    publisher = {ACM},
    title = {Cross-disciplinary Perspectives on Meta-learning for Algorithm Selection},
    url = {http://doi.acm.org/10.1145/1456650.1456656},
    volume = {41},
    year = {2009}
}

@phdthesis{Guo2003,
    author = {Guo, Haipeng},
    citeulike-article-id = {13113288},
    keywords = {ai, algorithm\_selection, bayesian, inference, machine\_learning, metacognition, metareasoning, sorting, strategy\_selection},
    posted-at = {2014-03-21 07:48:11},
    priority = {4},
    school = {Kansas State University},
    title = {Algorithm selection for sorting and probabilistic inference: a machine learning-based approach},
    year = {2003}
}

@article{Dowker2014Young,
    author = {Dowker, Ann},
    citeulike-article-id = {13105226},
    citeulike-linkout-0 = {http://dx.doi.org/10.3389/fnhum.2013.00924},
    doi = {10.3389/fnhum.2013.00924},
    issn = {1662-5161},
    journal = {Frontiers in Human Neuroscience},
    keywords = {arithmetic, cognition, cognitive\_development, mental\_arithmetic, strategy\_selection},
    posted-at = {2014-03-12 23:13:26},
    priority = {2},
    title = {Young children's use of derived fact strategies for addition and subtraction},
    url = {http://dx.doi.org/10.3389/fnhum.2013.00924},
    volume = {7},
    year = {2014}
}

@article{Venkatraman2012Strategic,
    abstract = {Complex economic decisions – whether investing money for retirement or purchasing some new electronic gadget – often involve uncertainty about the likely consequences of our choices. Critical for resolving that uncertainty are strategic meta-decision processes, which allow people to simplify complex decision problems, evaluate outcomes against a variety of contexts, and flexibly match behavior to changes in the environment. In recent years, substantial research has implicated the dorsomedial prefrontal cortex (dmPFC) in the flexible control of behavior. However, nearly all such evidence comes from paradigms involving executive function or response selection, not complex decision-making. Here, we review evidence that demonstrates that the dmPFC contributes to strategic control in complex decision-making. This region contains a functional topography such that the posterior dmPFC supports response-related control, whereas the anterior dmPFC supports strategic control. Activation in the anterior dmPFC signals changes in how a decision problem is represented, which in turn can shape computational processes elsewhere in the brain. Based on these findings, we argue for both generalized contributions of the dmPFC to cognitive control, and specific computational roles for its subregions depending upon the task demands and context. We also contend that these strategic considerations are likely to be critical for decision-making in other domains, including interpersonal interactions in social settings.},
    author = {Venkatraman, Vinod and Huettel, Scott A.},
    citeulike-article-id = {13097875},
    citeulike-linkout-0 = {http://dx.doi.org/10.1111/j.1460-9568.2012.08009.x},
    day = {1},
    doi = {10.1111/j.1460-9568.2012.08009.x},
    journal = {European Journal of Neuroscience},
    keywords = {cognition, cognitive\_control, decision\_making, metacognition, metacontrol, pfc, strategy\_selection, uncertainty},
    month = apr,
    number = {7},
    pages = {1075--1082},
    posted-at = {2014-03-10 01:28:33},
    priority = {2},
    publisher = {Blackwell Publishing Ltd},
    title = {Strategic control in decision-making under uncertainty},
    url = {http://dx.doi.org/10.1111/j.1460-9568.2012.08009.x},
    volume = {35},
    year = {2012}
}

@article{Reder1987Strategy,
    abstract = {There are multiple strategies for answering questions. For example, a statement is sometimes verified using a plausibility process and sometimes using a direct retrieval process. It is claimed that there is a distinct strategy selection phase and a framework is proposed to account for strategy selection. Six experiments support the assumptions of the proposed framework: The first three experiments show that strategy selection is under the strategic control of the subjects. These experiments also indicate what contextual variables affect this selection. Experiments 4 and 5 suggest that strategy selection also involves evaluating the question itself, while Experiment 6 suggests variables that influence the evaluation of the question. This model is shown to be consistent with processing strategies in domains other than question answering, viz., dual-task monitoring in divided attention situations.},
    author = {Reder, Lynne M.},
    citeulike-article-id = {12614191},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/0010-0285(87)90005-3},
    doi = {10.1016/0010-0285(87)90005-3},
    issn = {00100285},
    journal = {Cognitive Psychology},
    keywords = {cognition, judgment, memory, metacognition, strategy\_selection},
    month = jan,
    number = {1},
    pages = {90--138},
    posted-at = {2014-03-07 03:08:25},
    priority = {2},
    title = {Strategy selection in question answering},
    url = {http://dx.doi.org/10.1016/0010-0285(87)90005-3},
    volume = {19},
    year = {1987}
}

@article{Siegler1999,
    abstract = {Strategic development is more diverse, multifaceted and eventful than previously realized. Individual children know and use multiple strategies for solving a given kind of problem; they choose adaptively among available alternatives; and they frequently discover new strategies that enhance their problem-solving abilities. A recently formulated computer simulation (SCADS) indicates how associative and metacognitive processes work together to produce adaptive choices among existing strategies and discovery of useful new approaches.},
    author = {Siegler, Robert S.},
    citeulike-article-id = {828770},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/s1364-6613(99)01372-8},
    citeulike-linkout-1 = {http://www.sciencedirect.com/science/article/B6VH9-3XR2GXK-B/2/1943fa61308e745c16ea114c80cf2ad1},
    day = {1},
    doi = {10.1016/s1364-6613(99)01372-8},
    issn = {13646613},
    journal = {Trends in Cognitive Sciences},
    keywords = {cognition, cognitive\_development, learning, metacognition, strategy\_selection},
    month = nov,
    number = {11},
    pages = {430--435},
    posted-at = {2014-03-02 09:19:33},
    priority = {2},
    title = {Strategic development},
    url = {http://dx.doi.org/10.1016/s1364-6613(99)01372-8},
    volume = {3},
    year = {1999}
}

@article{Shrager1998,
    abstract = {Preschoolers show surprising competence in choosing adaptively among alternative strategies and in discovering new approaches. The SCADS computer simulation illustrates how simple processes can generate this impressive competence. The model's behavior parallels data on children's addition in at least eight ways: It uses diverse strategies over prolonged periods of time, makes adaptive choices among strategies, discovers the same strategies as children, discovers strategies in the same sequence as children, makes discoveries without trial and error, makes discoveries without having experienced failure, narrowly generalizes new approaches, and generalizes more broadly following challenging problems. SCADS thus indicates plausible sources of young children's surprising competence at strategy choice and strategy discovery.},
    author = {Shrager, Jeff and Siegler, Robert S.},
    citeulike-article-id = {13076350},
    citeulike-linkout-0 = {http://dx.doi.org/10.1111/1467-9280.00076},
    citeulike-linkout-1 = {http://pss.sagepub.com/cgi/content/abstract/9/5/405},
    day = {1},
    doi = {10.1111/1467-9280.00076},
    journal = {Psychological Science},
    keywords = {cognition, cognitive\_development, cognitive\_plasticity, learning, metacognition, strategy\_selection},
    month = sep,
    number = {5},
    pages = {405--410},
    posted-at = {2014-03-02 07:28:15},
    priority = {2},
    title = {{SCADS}: A Model of Children's Strategy Choices and Strategy Discoveries},
    url = {http://dx.doi.org/10.1111/1467-9280.00076},
    volume = {9},
    year = {1998}
}

@article{Pachur2012Type,
    abstract = {
                In order to be adaptive, cognition requires knowledge about the statistical structure of the environment. We show that decision performance and the selection between cue-based and exemplar-based inference mechanisms can depend critically on how this knowledge is acquired. Two types of learning tasks are distinguished: learning by comparison, by which the decision maker learns which of two objects has a higher criterion value, and direct criterion learning, by which the decision maker learns an object's criterion value directly. In three experiments, participants were trained either with learning by comparison or with direct criterion learning and subsequently tested with paired-comparison, classification, and estimation tasks. Experiments 1 and 2 showed that although providing less information, learning by comparison led to better generalization (at test), both when generalizing to new objects and when the task format at test differed from the task format during training. Moreover, learning by comparison enabled participants to provide rather accurate continuous estimates. Computational modeling suggests that the advantage of learning by comparison is due to differences in strategy selection: whereas direct criterion learning fosters the reliance on exemplar processing, learning by comparison fosters cue-based mechanisms. The pattern in decision performance reversed when the task environment was changed from a linear (Experiments 1 and 2) to a nonlinear structure (Experiment 3), where direct criterion learning led to better decisions. Our results demonstrate the critical impact of learning conditions for the subsequent selection of decision strategies and highlight the key role of comparison processes in cognition.
                Copyright {\copyright} 2012 Elsevier Inc. All rights reserved.
            },
    author = {Pachur, Thorsten and Olsson, Henrik},
    citeulike-article-id = {10666721},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.cogpsych.2012.03.003},
    citeulike-linkout-1 = {http://view.ncbi.nlm.nih.gov/pubmed/22575684},
    citeulike-linkout-2 = {http://www.hubmed.org/display.cgi?uids=22575684},
    doi = {10.1016/j.cogpsych.2012.03.003},
    issn = {1095-5623},
    journal = {Cognitive psychology},
    keywords = {cognition, cognitive\_resources, cognitive\_strategies, decision\_making, learning, reinforcement\_learning, strategy\_selection},
    month = sep,
    number = {2},
    pages = {207--240},
    pmid = {22575684},
    posted-at = {2014-02-27 03:06:19},
    priority = {2},
    title = {Type of learning task impacts performance and strategy selection in decision making.},
    url = {http://dx.doi.org/10.1016/j.cogpsych.2012.03.003},
    volume = {65},
    year = {2012}
}

@article{Mata2010Learning,
    abstract = {
                Decision makers often have to learn from experience. In these situations, people must use the available feedback to select the appropriate decision strategy. How does the ability to select decision strategies on the basis of experience change with age? We examined younger and older adults' strategy selection learning in a probabilistic inference task using a computational model of strategy selection learning. Older adults showed poorer decision performance compared with younger adults. In particular, older adults performed poorly in an environment favoring the use of a more cognitively demanding strategy. The results suggest that the impact of cognitive aging on strategy selection learning depends on the structure of the decision environment.
                (c) 2010 APA, all rights reserved
            },
    author = {Mata, Rui and von Helversen, Bettina and Rieskamp, J\"{o}rg},
    citeulike-article-id = {13073442},
    citeulike-linkout-0 = {http://dx.doi.org/10.1037/a0018923},
    citeulike-linkout-1 = {http://view.ncbi.nlm.nih.gov/pubmed/20545415},
    citeulike-linkout-2 = {http://www.hubmed.org/display.cgi?uids=20545415},
    doi = {10.1037/a0018923},
    issn = {1939-1498},
    journal = {Psychology and aging},
    keywords = {adaptation, aging, cognition, decision-making, learning, reinforcement\_learning, strategy\_selection},
    month = jun,
    number = {2},
    pages = {299--309},
    pmid = {20545415},
    posted-at = {2014-02-27 03:05:08},
    priority = {2},
    title = {Learning to choose: Cognitive aging and strategy selection learning in decision making.},
    url = {http://dx.doi.org/10.1037/a0018923},
    volume = {25},
    year = {2010}
}

@article{Payne1988,
    author = {Payne, John W. and Bettman, James R. and Johnson, Eric J.},
    citeulike-article-id = {13057361},
    journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
    keywords = {cognition, decision\_making, learning, metacognition, strategy\_selection},
    number = {3},
    pages = {534},
    posted-at = {2014-02-22 06:51:38},
    priority = {2},
    publisher = {American Psychological Association},
    title = {Adaptive strategy selection in decision making.},
    volume = {14},
    year = {1988}
}

@article{Broder2006,
    abstract = {
                Decision routines unburden the cognitive capacity of the decision maker. In changing environments, however, routines may become maladaptive. In 2 experiments with a hypothetical stock market game (n = 241), the authors tested whether decision routines tend to persist at the level of decision strategies rather than at the level of options in strategy selection. The payoff structure of the task was changed after 80 decision trials, rendering a new strategy optimal with respect to expected payoff. Whereas most participants detected the appropriate strategy at the beginning of the task, they tended to retain it even when it was no longer optimal. A hint about a possible change had only a small influence on this maladaptive routine; a monetary incentive had none. Switching to a similar but not identical task relaxed the routine, but not much.
                Copyright 2006 APA, all rights reserved.
            },
    author = {Br\"{o}der, Arndt and Schiffer, Stefanie},
    citeulike-article-id = {12952148},
    citeulike-linkout-0 = {http://dx.doi.org/10.1037/0278-7393.32.4.904},
    citeulike-linkout-1 = {http://view.ncbi.nlm.nih.gov/pubmed/16822156},
    citeulike-linkout-2 = {http://www.hubmed.org/display.cgi?uids=16822156},
    doi = {10.1037/0278-7393.32.4.904},
    issn = {0278-7393},
    journal = {Journal of experimental psychology. Learning, memory, and cognition},
    keywords = {adaptation, cognition, cognitive\_control, decision\_making, heuristics, metacognition, rationality, resource-rationality, strategy\_selection},
    month = jul,
    number = {4},
    pages = {904--918},
    pmid = {16822156},
    posted-at = {2014-02-04 05:10:02},
    priority = {2},
    title = {Adaptive flexibility and maladaptive routines in selecting fast and frugal decision strategies.},
    url = {http://dx.doi.org/10.1037/0278-7393.32.4.904},
    volume = {32},
    year = {2006}
}

@article{Rieskamp2006,
    abstract = {The assumption that people possess a repertoire of strategies to solve the inference problems they face has been raised repeatedly. However, a computational model specifying how people select strategies from their repertoire is still lacking. The proposed strategy selection learning (SSL) theory predicts a strategy selection process on the basis of reinforcement learning. The theory assumes that individuals develop subjective expectations for the strategies they have and select strategies proportional to their expectations, which are then updated on the basis of subsequent experience. The learning assumption was supported in 4 experimental studies. Participants substantially improved their inferences through feedback. In all 4 studies, the best-performing strategy from the participants' repertoires most accurately predicted the inferences after sufficient learning opportunities. When testing SSL against 3 models representing extensions of SSL and against an exemplar model assuming a memory-based inference process, the authors found that SSL predicted the inferences most accurately.},
    author = {Rieskamp, J\"{o}rg and Otto, Philipp E.},
    citeulike-article-id = {771035},
    citeulike-linkout-0 = {http://dx.doi.org/10.1037/0096-3445.135.2.207},
    citeulike-linkout-1 = {http://view.ncbi.nlm.nih.gov/pubmed/16719651},
    citeulike-linkout-2 = {http://www.hubmed.org/display.cgi?uids=16719651},
    citeulike-linkout-3 = {http://www.sciencedirect.com/science/article/B6X07-4K663V1-5/2/09ac61fd8f698b0cad81369a59bc1c3a},
    doi = {10.1037/0096-3445.135.2.207},
    issn = {0096-3445},
    journal = {Journal of Experimental Psychology: General},
    keywords = {bounded-rationality, cognition, ecological\_rationality, heuristics, metacognition, metareasoning, rationality, reinforcement\_learning, resource-rationality, strategy\_selection},
    month = may,
    number = {2},
    pages = {207--236},
    pmid = {16719651},
    posted-at = {2014-02-04 04:53:28},
    priority = {2},
    title = {{SSL}: A Theory of How People Learn to Select Strategies.},
    url = {http://dx.doi.org/10.1037/0096-3445.135.2.207},
    volume = {135},
    year = {2006}
}

@article{Marewski2014,
  title={Strategy selection: An introduction to the modeling challenge},
  author={Marewski, Julian N and Link, Daniela},
  journal={Wiley Interdisciplinary Reviews: Cognitive Science},
  volume={5},
  number={1},
  pages={39--59},
  year={2014},
  publisher={Wiley Online Library}
}
@article{Siegler1988,
    author = {Siegler, Robert S.},
    citeulike-article-id = {12718254},
    citeulike-linkout-0 = {http://dx.doi.org/10.1037/0096-3445.117.3.258},
    doi = {10.1037/0096-3445.117.3.258},
    issn = {1939-2222},
    journal = {Journal of Experimental Psychology: General},
    keywords = {learning, metacognition, strategy\_selection},
    number = {3},
    pages = {258--275},
    posted-at = {2013-10-12 09:44:32},
    priority = {2},
    title = {Strategy choice procedures and the development of multiplication skill.},
    url = {http://dx.doi.org/10.1037/0096-3445.117.3.258},
    volume = {117},
    year = {1988}
}
@article{Rice1976,
  title={The Algorithm Selection Problem},
  author={Rice, John R},
  journal={Advances in Computers},
  volume={15},
  pages={65--118},
  year={1976}
}
@book{Gigerenzer2002,
  title={Bounded rationality: The adaptive toolbox},
  author={Gigerenzer, Gerd and Selten, Reinhard},
  year={2002},
  publisher={{MIT} Press}
}

@article{Russell1991,
    author = {Russell, S. and Wefald, E.},
    citeulike-article-id = {12730446},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/0004-3702(91)90015-c},
    doi = {10.1016/0004-3702(91)90015-c},
    issn = {00043702},
    journal = {Artificial Intelligence},
    keywords = {ai, metacognition, metareasoning},
    number = {1-3},
    pages = {361--395},
    posted-at = {2013-10-21 23:31:10},
    priority = {2},
    title = {Principles of metareasoning},
    url = {http://dx.doi.org/10.1016/0004-3702(91)90015-c},
    volume = {49},
    year = {1991}
}

@INPROCEEDINGS{Hay2012,
  publisher = {AUAI Press},
  year = {2012},
  editor = {de Freitas, Nando and Murphy, Kevin},
  author = {Hay, NJ and Russell, S and Tolpin, D and Shimony, SE},
  title = {Selecting Computations: Theory and Applications},
  booktitle = {Uncertainty in Artificial Intelligence: Proceedings of the Twenty-Eighth Conference},
    address = {P.O. Box 866 Corvallis, Oregon 97339 USA}
}

@article{Gigerenzer2011,
    abstract = {As reflected in the amount of controversy, few areas in psychology have undergone such dramatic conceptual changes in the past decade as the emerging science of heuristics. Heuristics are efficient cognitive processes, conscious or unconscious, that ignore part of the information. Because using heuristics saves effort, the classical view has been that heuristic decisions imply greater errors than do  ” rational” decisions as defined by logic or statistical models. However, for many decisions, the assumptions of rational models are not met, and it is an empirical rather than an a priori issue how well cognitive heuristics function in an uncertain world. To answer both the descriptive question ( ” Which heuristics do people use in which situations?”) and the prescriptive question ( ” When should people rely on a given heuristic rather than a complex strategy to make better judgments?”), formal models are indispensable. We review research that tests formal models of heuristic inference, including in business organizations, health care, and legal institutions. This research indicates that (a) individuals and organizations often rely on simple heuristics in an adaptive way, and (b) ignoring part of the information can lead to more accurate judgments than weighting and adding all information, for instance for low predictability and small samples. The big future challenge is to develop a systematic theory of the building blocks of heuristics as well as the core capacities and environmental structures these exploit.},
    author = {Gigerenzer, Gerd and Gaissmaier, Wolfgang},
    citeulike-article-id = {8984735},
    citeulike-linkout-0 = {http://www.annualreviews.org/doi/abs/10.1146/annurev-psych-120709-145346},
    citeulike-linkout-1 = {http://dx.doi.org/10.1146/annurev-psych-120709-145346},
    citeulike-linkout-2 = {http://view.ncbi.nlm.nih.gov/pubmed/21126183},
    citeulike-linkout-3 = {http://www.hubmed.org/display.cgi?uids=21126183},
    doi = {10.1146/annurev-psych-120709-145346},
    issn = {1545-2085},
    journal = {Annual Review of Psychology},
    keywords = {decision-making, decision\_making, heuristics, psychology},
    number = {1},
    pages = {451--482},
    pmid = {21126183},
    posted-at = {2011-03-13 22:39:01},
    priority = {2},
    title = {Heuristic Decision Making},
    volume = {62},
    year = {2011}
}

@article{Erev2005,
    abstract = {
                Analysis of binary choice behavior in iterated tasks with immediate feedback reveals robust deviations from maximization that can be described as indications of 3 effects: (a) a payoff variability effect, in which high payoff variability seems to move choice behavior toward random choice; (b) underweighting of rare events, in which alternatives that yield the best payoffs most of the time are attractive even when they are associated with a lower expected return; and (c) loss aversion, in which alternatives that minimize the probability of losses can be more attractive than those that maximize expected payoffs. The results are closer to probability matching than to maximization. Best approximation is provided with a model of reinforcement learning among cognitive strategies (RELACS). This model captures the 3 deviations, the learning curves, and the effect of information on uncertainty avoidance. It outperforms other models in fitting the data and in predicting behavior in other experiments.
                Copyright (c) 2005 APA, all rights reserved.
            },
    address = {Max Werthiemer Minerva Center for Cognitive Studies, Faculty of Industrial Engineering and Management, Technion-Israel Institute of Technology, Haifa, Israel. erev@tx.technion.ac.il},
    author = {Erev, Ido and Barron, Greg},
    citeulike-article-id = {876389},
    citeulike-linkout-0 = {http://dx.doi.org/10.1037/0033-295x.112.4.912},
    citeulike-linkout-1 = {http://view.ncbi.nlm.nih.gov/pubmed/16262473},
    citeulike-linkout-2 = {http://www.hubmed.org/display.cgi?uids=16262473},
    doi = {10.1037/0033-295x.112.4.912},
    issn = {0033-295X},
    journal = {Psychological review},
    keywords = {learning, metacognition, probability\_matching, reinforcement\_learning, strategy\_selection, strategy\_selection\_learning},
    month = oct,
    number = {4},
    pages = {912--931},
    pmid = {16262473},
    posted-at = {2012-04-05 03:25:52},
    priority = {2},
    title = {On adaptation, maximization, and reinforcement learning among cognitive strategies.},
    url = {http://dx.doi.org/10.1037/0033-295x.112.4.912},
    volume = {112},
    year = {2005}
}

@inproceedings{Jaakkola1997,
  title={A variational approach to {B}ayesian logistic regression models and their extensions},
  author={Jaakkola, T and Jordan, M},
  booktitle={Sixth International Workshop on Artificial Intelligence and Statistics},
  year={1997}
}

@article{Kass1995,
    abstract = {In a 1935 paper and in his book Theory of probability, Jeffresy developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpies was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P-values, less attention has been given to the Bayes as a practical tool of applied statistics. In this article we review and discuss the uses of Bayes factors in the context of five scientific applications in genetics, sports, ecology, sociology, and psychology. We emphasize the following points: From Jeffrey's Bayesian viewpoint, the purpose of hypothesis testing is to evaluate the evidence in favor of a scientific theory. Bayes factors offer a way of evaluating evidence in favor of a null hypothesis. Bayes factors provide a way of incorporating external information into the evaluation of evidence about a hypothesis. Bayes factors are very general and do not require alternative models to be nested. Several techniques are available for computing Bayes factors, including asymptotic approximations that are easy to compute using the output from standard packages that maximize likelihoods. In "non-Bayesian significance tests. The Schwarz criterion (or BIC) gives a rough approximation to the logarithm of the Bayes factor, which is easy to use and does not require evaluation of prior distributions. When one is interested in estimation or prediction, Bayes factors may be converted to weights to be attached to various models so that a composite estimate or prediction may be obtained that takes account of structural or model uncertainty. Algorithms have been proposed that allow model uncertainty to be taken into account when the class of models initially considered is very large. Bayes factors are useful for guiding an evolutionary model-building process. It is important, and feasible, to assess the sensitivity of conclusions to the prior distributions used.},
    author = {Kass, Robert E. and Raftery, Adrian E.},
    citeulike-article-id = {1778922},
    citeulike-linkout-0 = {http://dx.doi.org/10.2307/2291091},
    citeulike-linkout-1 = {http://www.jstor.org/stable/2291091},
    doi = {10.2307/2291091},
    issn = {01621459},
    journal = {Journal of the American Statistical Association},
    keywords = {bayesian, bayesian\_model\_selection, bayesian\_statistics, model\_selection},
    month = jun,
    number = {430},
    pages = {773--795},
    posted-at = {2012-06-17 12:19:21},
    priority = {2},
    publisher = {American Statistical Association},
    title = {Bayes Factors},
    url = {http://dx.doi.org/10.2307/2291091},
    volume = {90},
    year = {1995}
}

@article{Penny2013,
    abstract = {Statistical Parametric Mapping (SPM) is the dominant paradigm for mass-univariate analysis of neuroimaging data. More recently, a Bayesian approach termed Posterior Probability Mapping (PPM) has been proposed as an alternative. PPM offers two advantages: (i) inferences can be made about effect size thus lending a precise physiological meaning to activated regions, (ii) regions can be declared inactive. This latter facility is most parsimoniously provided by PPMs based on Bayesian model comparisons. To date these comparisons have been implemented by an Independent Model Optimization (IMO) procedure which separately fits null and alternative models. This paper proposes a more computationally efficient procedure based on Savage-Dickey approximations to the Bayes factor, and Taylor-series approximations to the voxel-wise posterior covariance matrices. Simulations show the accuracy of this Savage-Dickey-Taylor (SDT) method to be comparable to that of IMO. Results on fMRI data show excellent agreement between SDT and IMO for second-level models, and reasonable agreement for first-level models. This Savage-Dickey test is a Bayesian analogue of the classical SPM-F and allows users to implement model comparison in a truly interactive manner.},
    author = {Penny, William D. and Ridgway, Gerard R.},
    citeulike-article-id = {12794649},
    citeulike-linkout-0 = {http://dx.doi.org/10.1371/journal.pone.0059655},
    doi = {10.1371/journal.pone.0059655},
    journal = {PLoS ONE},
    keywords = {bayesian, bayesian\_inference, bayesian\_model\_selection, bayesian\_statistics, model\_selection},
    number = {3},
    pages = {e59655+},
    posted-at = {2013-11-20 03:38:53},
    priority = {2},
    publisher = {Public Library of Science},
    title = {Efficient Posterior Probability Mapping Using {Savage-Dickey} Ratios},
    url = {http://dx.doi.org/10.1371/journal.pone.0059655},
    volume = {8},
    year = {2013}
}